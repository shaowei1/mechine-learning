# 决策树- 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。- 缺点：可能会产生过度匹配问题。    - 适用数据类型：数值型和标称型。# 概述- 熵：表示随机变量的不确定性。- 条件熵：在一个条件下，随机变量的不确定性。- 信息增益：熵 - 条件熵在一个条件下，信息不确定性减少的程度！通俗地讲，X(明天下雨)是一个随机变量，X的熵可以算出来， Y(明天阴天)也是随机变量， 在阴天情况下下雨的信息熵我们如果也知道的话（此处需要知道其联合概率分布或是通过数据估计）即是条件熵。 两者相减就是信息增益！原来明天下雨例如信息熵是2，条件熵是0.01（因为如果是阴天就下雨的概率很大， 信息就少了），这样相减后为1.99，在获得阴天这个信息后， 下雨信息不确定性减少了1.99！是很多的！ 所以信息增益大！也就是说，阴天这个信息对下雨来说是很重要的！ 所以在特征选择的时候常常用信息增益，如果IG（信息增益大）的话那么这个特征对于分类来说很关键~~ 决策树就是这样来找特征的！是最常用的数据挖掘算法，决策树允许机器根据数据集创造规则，其实这就是机器学习的过程。专家系统中经常会使用到决策树及其变种，一、首先编写几个函数对数据进行预处理：1.计算熵函数：熵代表集合的无序程度，集合越分布无序，则熵越大；```pythonfrom math import logdef entropy(sample):    log2 = lambda x:log(x)/log(2)        results = {}    for row in sample:        r = row[len(row) - 1]        results[r] = results.get(r, 0) + 1        ent = 0.0 # entropy    for r in results.keys():        p = float(results[r])/len(sample)        ent -= p * log2(p)    return ent```    ```python# createBranch()检测数据集中的每个子项是否属于同一分类：    If so         return 类标签；    Else            寻找划分数据集的最好特征            划分数据集            创建分支节点                for 每个划分的子集                    调用函数createBranch并增加返回结果到分支节点中            return 分支节点```    ## 一般流程- 收集数据：可以使用任何方法。- 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。- 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。- 训练算法：构造树的数据结构。- 测试算法：使用经验树计算错误率。- 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，我们首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的所有数据属于同一分类。ID3算法可以用于划分标称型数据集。构建决策树时，我们通常采用递归的方法将数据集转化为决策树。一般我们并不构造新的数据结构，而是使用Python语言内嵌的数据结构字典存储树节点信息。使用Matplotlib的注解功能，我们可以将存储的树结构转化为容易理解的图形。Python语言的pickle模块可用于存储决策树的结构。隐形眼镜的例子表明决策树可能会产生过多的数据集划分，从而产生过度匹配数据集的问题。我们可以通过裁剪决策树，合并相邻的无法产生大量信息增益的叶节点，消除过度匹配问题。还有其他的决策树的构造算法，最流行的是C4.5和CART，第9章讨论回归问题时将介绍CART算法。本书第2章、第3章讨论的是结果确定的分类算法，数据实例最终会被明确划分到某个分类中。下一章我们讨论的分类算法将不能完全确定数据实例应该划分到某个分类，或者只能给出数据实例属于给定分类的概率。 # 引用 在信息论中，熵被用来衡量一个随机变量出现的期望值。变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大，熵是整个系统的平均消息量。 信息熵是信息论中用于度量信息量的一个概念。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以，信息熵也可以说是系统有序化程度的一个度量。 信息熵计算公式是：H(x)=E[I(xi)]=E[ log(1/p(xi)) ]=-∑p(xi)log(p(xi))(i=1,2,..n)。自信息，又称信息本体，用来衡量单一事件发生时所包含的信息量的多少。如果事件发生的机率是P(x)，则信息本体I(x)的定义就是：-log（P(x)）。互信息（Mutual Information）是一有用的信息度量，它是指两个事件集合之间的相关性。（PMI）        在信息增益中，衡量标准是看特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。对一个特征而言，系统有它和没它时信息量将发生变化，而前后信息量的差值就是这个特征给系统带来的信息量。所谓信息量，其实就是熵。信息增益的计算过程如下： 1、 计算熵：熵越高，混合的数据也越多。我们检查的属性是是否出去玩。用Excel对上面数据的play变量的各个取值排个序（这个工作簿里把“play”这个词去掉），一共是14条记录，你能数出取值为yes的记录有9个，取值为no的有5个，我们说这个样本里有9个正例，5 个负例，记为S(9+,5-)，S是样本的意思(Sample)。这里熵记为Entropy(S),计算公式为：Entropy(S)= -(9/14)*log(9/14)-(5/14)*log(5/14)