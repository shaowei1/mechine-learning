朴素贝叶斯对电子邮件进行分类- 收集数据- 准备数据: 将文本文件解析成词条向量- 分析数据: 检查词条确保解析的正确性- 训练算法: trainNB0()- 测试算法: 使用classifyNB(), 并且构建一个新的测试函数来计算文档的错误率- 下溢问题: 对概率取对数- 词袋模型: 解决文档分类问题上比词集模型有所提高- 移除停用词- 切分器优化贝叶斯决策理论的核心思想是：选择高概率所对应的类别，选择具有最高概率的决策。有时也被总结成“多数占优”的原则。### 期望一个离散性随机变量的期望值（或数学期望、或均值，亦简称期望，物理学中称为期待值）是试验中每次可能的结果乘以其结果概率的总和。### 熵2 bit的熵。在信息论中，熵（英语：entropy）是接收的每条消息中包含的信息的平均量，又被稱為信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）来自信源的另一个特征是样本的概率分布。这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的信息。由于一些其他的原因，把信息（熵）定义为概率分布的对数的相反数是有道理的。事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望）就是这个分布产生的信息量的平均值（即熵）。熵的单位通常为比特，但也用Sh、nat、Hart计量，取决于定义用到对数的底。采用概率分布的对数作为信息的量度的原因是其可加性。例如，投掷一次硬币提供了1 Sh的信息，而掷m次就为m位。更一般地，你需要用log2(n)位来表示一个可以取n个值的变量。#### 熵的计算如果有一枚理想的硬币，其出现正面和反面的机会相等，则抛硬币事件的熵等于其能够达到的最大值。我们无法知道下一个硬币抛掷的结果是什么，因此每一次抛硬币都是不可预测的。因此，使用一枚正常硬币进行若干次抛掷，这个事件的熵是一比特，因为结果不外乎两个——正面或者反面，可以表示为0, 1编码，而且两个结果彼此之间相互独立。若进行n次独立实验，则熵为n，因为可以用长度为n的比特流表示。但是如果一枚硬币的两面完全相同，那个这个系列抛硬币事件的熵等于零，因为结果能被准确预测。现实世界里，我们收集到的数据的熵介于上面两种情况之间。另一个稍微复杂的例子是假设一个随机变量X，取三种可能值x1,x2,x3, 概率分别是0.5, 0.25, 0.25 ，那么编码的bit长度是0.5\*1 + 0.25\*2+ 0.25\*2，　其熵为3/2因此熵实际是对随机变量的比特量和顺次发生概率相乘再总和的数学期望。